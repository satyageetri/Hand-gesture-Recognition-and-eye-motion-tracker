# HAND GESTURE RECOGNITION AND EYE MOTION TRACKER
The primary goal of Human-Computer Interaction is to improve the interaction between users and computers by making the computer more receptive to user needs. Human interaction comes from different sensory modes like gesture, speech, facial and body expressions. Being able to interact with the system naturally is becoming ever more important in many fields of Human-Computer Interaction. An individual human-computer interface system using eye movements and hand gestures is introduced. Traditionally, the human-computer interface uses a mouse,  keyboards, and input devices.

Deaf and Dumb people in our society face a huge Communication Barrier.
Our primary goal is to implement the required set of signs in sign language for understanding the sign and displaying the actual meaning of the sign.

<br><br>
<img src="https://github.com/satyageetri/Hand-gesture-Recognition-and-eye-motion-tracker/assets/88814765/4af60989-4e5e-40ea-8276-ae48fcf0147e" width="400" height="300" >
<img src ="https://github.com/satyageetri/Hand-gesture-Recognition-and-eye-motion-tracker/assets/88814765/aa5c9dd5-adab-4b49-967f-1eed46505a92" width="400" height="300"><br>
<br>
### ARCHITECTURE OF HAND GESTURE RECOGNITION AND EYE MOTION TRACKER SYSTEM
<br><br>

# REQUIREMENTS
* mediapipe 0.8.1
* OpenCV 3.4.2 or Later
* Tensorflow 2.3.0 or Later<br>tf-nightly 2.5.0.dev or later (Only when creating a TFLite for an LSTM model)
* scikit-learn 0.23.2 or Later (Only if you want to display the confusion matrix) 
* matplotlib 3.3.2 or Later (Only if you want to display the confusion matrix)

### app.py
This is a sample program for inference.<br>
In addition, learning data (key points) for hand sign recognition,<br>
You can also collect training data (index finger coordinate history) for finger gesture recognition.

### keypoint_classification.ipynb
This is a model training script for hand sign recognition.

### point_history_classification.ipynb
This is a model training script for finger gesture recognition.
<br><br><br><br>
![image](https://github.com/satyageetri/Hand-gesture-Recognition-and-eye-motion-tracker/assets/88814765/59e39296-559a-45a1-bc90-304fc6e06ff3)
<br><br><br><br>

# RESULTS
<img src="https://github.com/satyageetri/Hand-gesture-Recognition-and-eye-motion-tracker/assets/88814765/e059efc2-15fe-4e11-9784-64ba88208309" width="400" height="300" >
<img src="https://github.com/satyageetri/Hand-gesture-Recognition-and-eye-motion-tracker/assets/88814765/bd90e742-39c2-4f70-abcf-7ed2409a3a7b" width="400" height="300" >
<img src="https://github.com/satyageetri/Hand-gesture-Recognition-and-eye-motion-tracker/assets/88814765/6598694c-3f49-4d9a-b34f-269291ac96ef" width="400" height="300" >
<img src="https://github.com/satyageetri/Hand-gesture-Recognition-and-eye-motion-tracker/assets/88814765/ed927ba0-b7b1-4bca-a8a2-891f043f545d" width="400" height="300" >
<img src="https://github.com/satyageetri/Hand-gesture-Recognition-and-eye-motion-tracker/assets/88814765/ff865771-6555-416f-9af5-078f56c4bf1c" width="400" height="300" >
<img src="https://github.com/satyageetri/Hand-gesture-Recognition-and-eye-motion-tracker/assets/88814765/49823f63-bba0-4690-9bb3-8abe93c9b743" width="400" height="300" >

